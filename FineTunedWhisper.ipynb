{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf66b9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!add-apt-repository -y ppa:jonathonf/ffmpeg-4 -q\n",
    "!apt update -q\n",
    "!apt install -y ffmpeg -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d16c20",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q \"datasets>=2.6.1\"\n",
    "!pip install -q \"evaluate>=0.3.0\"\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main\n",
    "!pip install -q librosa\n",
    "!pip install -q jiwer\n",
    "!pip install -q gradio\n",
    "!pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/evaluate.git@main\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aede4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abd9cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model_name_or_path = \"openai/whisper-small\"\n",
    "language =\"English\"\n",
    "language_abbr =\"en\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"Trelis//llm-lingo\"\n",
    "org = \"Trelis\"\n",
    "trained_adapter_name = \"whisper-small-llm-lingo-adapters\"\n",
    "trained_model_name = \"whisper-small-llm-lingo\"\n",
    "trained_adapter_repo = org + \"/\" + trained_adapter_name\n",
    "trained_model_repo = org + '/' + trained_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb48872",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "!pip install moviepy\n",
    "\n",
    "from moviepy.editor import *\n",
    "\n",
    "video = VideoFileClip(\"train1.mp4\")\n",
    "video.audio.write_audiofile(\"train1.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683d749",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "!pip install moviepy\n",
    "\n",
    "from moviepy.editor import *\n",
    "\n",
    "video = VideoFileClip(\"validation_MyVoice.mp4\")\n",
    "video.audio.write_audiofile(\"validation_MyVoice.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55f1d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets librosa torchaudio\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import (\n",
    "    AutomaticSpeechRecognitionPipeline,\n",
    "    WhisperTimeStampLogitsProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598a21c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-small\"\n",
    "\n",
    "\n",
    "whisper_asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_name_or_path,\n",
    "    chunk_length_s=30,\n",
    "    ignore_warning=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917add6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    if seconds is None:\n",
    "        return \"00:00:00.000\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = seconds % 60\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:06.3f}\".replace('.', ',')\n",
    "\n",
    "\n",
    "def process_audio_and_create_vtt(audio_filename, audio_type, whisper_asr, output_filename=None):\n",
    "    \"\"\"\n",
    "    Generate VTT subtitle file from audio using Whisper ASR.\n",
    "\n",
    "    Parameters:\n",
    "    - audio_filename: base name (without extension)\n",
    "    - audio_type: e.g., 'mp3', 'mp4', 'wav'\n",
    "    - whisper_asr: pipeline object from Hugging Face\n",
    "    - output_filename: optional output filename (e.g., 'output.vtt')\n",
    "    \"\"\"\n",
    "\n",
    "    prediction = whisper_asr(f\"{audio_filename}.{audio_type}\", return_timestamps=True)\n",
    "\n",
    "\n",
    "    vtt_file_name = output_filename if output_filename else f\"{audio_filename}.vtt\"\n",
    "\n",
    "\n",
    "    with open(vtt_file_name, \"w\", encoding='utf-8') as vtt_file:\n",
    "        vtt_file.write(\"WEBVTT\\n\\n\")\n",
    "        for chunk in prediction.get(\"chunks\", []):\n",
    "\n",
    "            start, end = chunk.get(\"timestamp\", (None, None))\n",
    "\n",
    "\n",
    "            if start is None or end is None:\n",
    "                continue\n",
    "\n",
    "\n",
    "            start_time = format_time(start)\n",
    "            end_time = format_time(end)\n",
    "            text = chunk.get(\"text\", \"\").strip()\n",
    "\n",
    "            vtt_file.write(f\"{start_time} --> {end_time}\\n{text}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0390d13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "process_audio_and_create_vtt(\"train1\", \"mp3\", whisper_asr)\n",
    "process_audio_and_create_vtt(\"validation_MyVoice\",\"mp3\",whisper_asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ad342",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/datasets/Trelis/llm-lingo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884d415",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"llm-lingo\"):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78508b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Audio\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df = pd.read_parquet(\"llm-lingo/data/train-00000-of-00001.parquet\")\n",
    "val_df = pd.read_parquet(\"llm-lingo/data/validation-00000-of-00001.parquet\")\n",
    "\n",
    "\n",
    "print(f\" Train rows: {len(train_df)}\")\n",
    "print(f\" Validation rows: {len(val_df)}\")\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio())\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio())\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "\n",
    "print(\"\\n Dataset loaded:\")\n",
    "print(dataset)\n",
    "\n",
    "print(\"\\n All Train Samples:\")\n",
    "for i, sample in enumerate(dataset[\"train\"]):\n",
    "    print(f\"Train {i+1}: {sample['audio']['path']} — Text: {sample.get('text', 'No text')}\")\n",
    "\n",
    "\n",
    "print(\"\\n All Validation Samples:\")\n",
    "for i, sample in enumerate(dataset[\"validation\"]):\n",
    "    print(f\"Validation {i+1}: {sample['audio']['path']} — Text: {sample.get('text', 'No text')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e8e46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ae768",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b41270f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff4063",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd39983",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa8708",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f54e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
